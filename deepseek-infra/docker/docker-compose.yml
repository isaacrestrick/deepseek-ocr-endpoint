version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:nightly
    container_name: deepseek-ocr
    runtime: nvidia

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/model-cache
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - TOKEN_COMPRESSION_MODE=${TOKEN_COMPRESSION_MODE:-base}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.9}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-4096}

    volumes:
      - ./model-cache:/model-cache

    ports:
      - "8000:8000"

    command: >
      --model deepseek-ai/DeepSeek-OCR
      --trust-remote-code
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.9}
      --max-model-len ${MAX_MODEL_LEN:-4096}

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

# Alternative: Build custom image from Dockerfile
#   build:
#     context: .
#     dockerfile: Dockerfile
#   image: deepseek-ocr:latest
